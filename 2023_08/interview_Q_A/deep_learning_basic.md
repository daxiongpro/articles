# 深度学习基础

## 反向传播

Q：为什么训练比推理需要消耗更多的显存？

A：反向传播算法会重复利用前向传播中存储的中间值。比如 $y = sigmoid(x) =  \frac{1}{1+e^{-x}}$ 的导数为 $y' = y * (1-y) $，y 值就是中间值。

## Batch 梯度下降

Q：剃度下降分为哪几类？

A：批量梯度下降(BGD)、随机梯度下降(SGD)、小批量梯度下降(MBGD)。批量是整个训练集，随机是单个样本，小批量是有 batchsize。随机可能会不收敛，迭代次数增多。

Q：盲目增大 Batch_Size 有何坏处？

A：有以下坏处：

1. 显存容量可能不够。
2. 跑完一次 epoch（全数据集）所需的迭代次数减少，要想达到相同的精度，其所花费的时间大大增加了，从而对参数的修正也就显得更加缓慢。
3. Batch_Size 增大到一定程度，其确定的下降方向已经基本不再变化。最终收敛精度会陷入局部极值，达到最终收敛精度上的最优。

## 激活函数

Q：为什么激活函数能让神经网络能拟合任何函数？

A：分段线性函数能逼近任何函数，激活函数让神经网络具备了“分段函数”的能力，所以神经网络能逼近任何函数。比如 Relu激活函数在输入x小于0时，输出y为0，即实现了截断（关闭）功能，在x大于等于0时，输出y与输入x相等，即完全的导通（打开）功能。

> sigmoid 是阶跃函数的"软化"。阶跃函数：$f(x) =  \left\{\begin{aligned}1, \quad x>0 \\ 0.5, \quad x=0 \\ 0, \quad x<0  \end{aligned} \right. $，使用 sigmoid 是因为求导方便。

Q：如何选择激活函数？

A：以下是常见的选择情况：

1. sigmoid 激活函数：除了输出层是一个二分类问题基本不会用它。
2. Softmax函数：一般作为神经网络的最后一层，接受来自上一层网络的输入值，然后将其转化为概率。
3. tanh 激活函数：整个函数是以 0 为中心的，即他本身是零均值的，在前向传播过程中，输入数据的均值并不会发生改变，这就使他在很多应用中效果能比 Sigmoid 优异一些。
4. ReLu 激活函数：最常用的默认函数，如果不确定用哪个激活函数，就使用 ReLu 。函数在 x > 0 时导数为 1，在一定程度上缓解了神经网络的梯度消失问题，加速梯度下降的收敛速度。缺点：ReLU 函数的输出是非零中心化的，给后一层的神经网络引入偏置偏移，会 **影响梯度下降的效率** 。ReLU 神经元在训练时比较容易“死亡”。如果神经元参数值在一次不恰当的更新后，其值小于 0，那么这个神经元自身参数的梯度永远都会是 0，在以后的训练过程中永远不能被激活，这种现象被称作“ **死区** ”，可以使用 Leaky ReLU 函数。

## 损失函数

Q：损失函数、代价函数、目标函数三个名词有何不同？

A：有以下区别：

* **损失函数** （`loss function`）: 用于定义单个训练样本预测值与真实值之间的误差
* **代价函数** （`cost function`）: 用于定义单个批次/整个训练集样本预测值与真实值之间的累计误差。
* **目标函数** （`objective function`）: 泛指任意可以被优化的函数





## 卷积神经网络

Q：如何计算卷积神经网络输出值？

A：图像大小、步幅和卷积后的Feature Map大小是有关系的。它们满足下面的关系：

$$
W_2 = (W_1 - F + 2P)/S + 1\\
H_2 = (H_1 - F + 2P)/S + 1
$$

其中$ W_2 $， 是卷积后 Feature Map 的宽度；$ W_1 $ 是卷积前图像的宽度；$ F $ 是 filter 的宽度；$ P $ 是 Zero Padding 数量，Zero Padding 是指在原始图像周围补几圈 $0$，如果 $P$ 的值是 $1$，那么就补 $1$ 圈 $0$；$S$ 是步幅；$ H_2 $ 卷积后 Feature Map 的高度；$ H_1 $ 是卷积前图像的宽度。
