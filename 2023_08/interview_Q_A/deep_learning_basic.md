# 深度学习基础

## 反向传播

Q：为什么训练比推理需要消耗更多的显存？

A：反向传播算法会重复利用前向传播中存储的中间值。比如 $y = sigmoid(x) =  \frac{1}{1+e^{-x}}$ 的导数为 $y' = y * (1-y) $，y 值就是中间值。

## 优化算法

Q：梯度下降分为哪几类？

A：批量梯度下降(BGD)、随机梯度下降(SGD)、小批量梯度下降(MBGD)。批量是整个训练集，随机是单个样本，小批量是有 batchsize。随机可能会不收敛，迭代次数增多。

Q：盲目增大 Batch_Size 有何坏处？

A：有以下坏处：

1. 显存容量可能不够。
2. 跑完一次 epoch（全数据集）所需的迭代次数减少，要想达到相同的精度，其所花费的时间大大增加了，从而对参数的修正也就显得更加缓慢。
3. Batch_Size 增大到一定程度，其确定的下降方向已经基本不再变化。最终收敛精度会陷入局部极值，达到最终收敛精度上的最优。

## 归一化

Q：为什么要归一化？

A：原因：

* 为了程序运行时收敛加快
* 避免梯度消失

Q：为什么归一化能提高求解最优解速度？

A：未归一化时，等高线为椭圆。很有可能走“之字型”路线（垂直等高线走）

Q：规范化分为哪些？

A：有以下几类：

* 0-1规范化：$\frac{x-min}{max-min} $。比较适用在数值比较集中的情况。当有新数据加入时，可能会导致最大值最小值发生变化，需要重新计算。
* Z-Score：$\frac{x-\mu}{\sigma}$。对原始数据的分布有要求，要求原始数据数据分布为正太分布计算。

Q：什么是批归一化（Batch Normalization）

A：以前在神经网络训练中，只是对输入层数据进行归一化处理，却没有在中间层进行归一化处理。虽然我们对输入数据进行了归一化处理，但是输入数据经过 $ \sigma(WX+b) $ 这样的矩阵乘法以及非线性运算之后，其数据分布很可能被改变，而随着深度网络的多层运算之后，数据分布的变化将越来越大。如果在网络的中间也进行归一化处理，能对网络的训练起到改进作用。 这种在神经网络中间层也进行归一化处理，使训练效果更好的方法，就是批归一化Batch Normalization（BN）。

Q：批归一化（BN）算法流程

A：下面给出 BN 算法在训练时的过程

输入：上一层输出结果 $ X = {x_1, x_2, ..., x_m} $，学习参数 $ \gamma, \beta $

算法流程：

1. 计算上一层输出数据的均值

$$
\mu_{\beta} = \frac{1}{m} \sum_{i=1}^m(x_i)
$$

其中，$ m $ 是此次训练样本 batch 的大小。

2. 计算上一层输出数据的标准差

$$
\sigma_{\beta}^2 = \frac{1}{m} \sum_{i=1}^m (x_i - \mu_{\beta})^2
$$

3. 归一化处理，得到

$$
\hat x_i = \frac{x_i + \mu_{\beta}}{\sqrt{\sigma_{\beta}^2} + \epsilon}
$$

其中 $ \epsilon $ 是为了避免分母为 0 而加进去的接近于 0 的很小值

4. 重构，对经过上面归一化处理得到的数据进行重构，得到

$$
y_i = \gamma \hat x_i + \beta
$$

其中，$ \gamma, \beta $ 为可学习参数。

注：上述是 BN 训练时的过程，但是当在投入使用时，往往只是输入一个样本，没有所谓的均值 $ \mu_{\beta} $ 和标准差 $ \sigma_{\beta}^2 $。此时，均值 $ \mu_{\beta} $ 是计算所有 batch $ \mu_{\beta} $ 值的平均值得到，标准差 $ \sigma_{\beta}^2 $ 采用每个batch $ \sigma_{\beta}^2 $ 的无偏估计得到。

## 激活函数

Q：为什么激活函数能让神经网络能拟合任何函数？

A：分段线性函数能逼近任何函数，激活函数让神经网络具备了“分段函数”的能力，所以神经网络能逼近任何函数。比如 Relu激活函数在输入x小于0时，输出y为0，即实现了截断（关闭）功能，在x大于等于0时，输出y与输入x相等，即完全的导通（打开）功能。

> sigmoid 是阶跃函数的"软化"。阶跃函数：$f(x) =  \left\{\begin{aligned}1, \quad x>0 \\ 0.5, \quad x=0 \\ 0, \quad x<0  \end{aligned} \right. $，使用 sigmoid 是因为求导方便。

Q：如何选择激活函数？

A：以下是常见的选择情况：

1. sigmoid 激活函数：除了输出层是一个二分类问题基本不会用它。
2. Softmax函数：一般作为神经网络的最后一层，接受来自上一层网络的输入值，然后将其转化为概率。
3. tanh 激活函数：整个函数是以 0 为中心的，即他本身是零均值的，在前向传播过程中，输入数据的均值并不会发生改变，这就使他在很多应用中效果能比 Sigmoid 优异一些。
4. ReLu 激活函数：最常用的默认函数，如果不确定用哪个激活函数，就使用 ReLu 。函数在 x > 0 时导数为 1，在一定程度上缓解了神经网络的梯度消失问题，加速梯度下降的收敛速度。缺点：ReLU 函数的输出是非零中心化的，给后一层的神经网络引入偏置偏移，会影响梯度下降的效率 。ReLU 神经元在训练时比较容易“死亡”。如果神经元参数值在一次不恰当的更新后，其值小于 0，那么这个神经元自身参数的梯度永远都会是 0，在以后的训练过程中永远不能被激活，这种现象被称作“ **死区** ”，可以使用 Leaky ReLU 函数。

## 损失函数

Q：损失函数、代价函数、目标函数三个名词有何不同？

A：有以下区别：

* **损失函数** （`loss function`）: 用于定义单个训练样本预测值与真实值之间的误差
* **代价函数** （`cost function`）: 用于定义单个批次/整个训练集样本预测值与真实值之间的累计误差。
* **目标函数** （`objective function`）: 泛指任意可以被优化的函数

Q：为什么分类问题使用交叉熵损失函数？

A：交叉熵误差是用来评估模型输出的概率分布和真实的概率分布的差异。在计算过程中，只有真实类别对应的那一项会被计算在内，其他类别的项在求和过程中均为0。参考：[你真的理解交叉熵损失函数了吗？](https://www.bilibili.com/video/BV1mZ4y1R76t/?spm_id_from=333.337.search-card.all.click&vd_source=da7944bcc998e29818ec76ea9c6f1f47)

Q：为什么分类问题 不能使用 MSE，而要使用 cross entropy 损失？

A：非凸有多个极值点；当 sigmoid 函数和MSE一起使用时会出现梯度消失，因为 MSE loss 对 w 和 b 求导，会有f'(x) = sigmoid'(x)  = f(x) * [1 - f(x)] 一项。

## 卷积神经网络

Q：如何计算卷积神经网络输出值？

A：图像大小、步幅和卷积后的Feature Map大小是有关系的。它们满足下面的关系：

$$
W_2 = (W_1 - F + 2P)/S + 1\\
H_2 = (H_1 - F + 2P)/S + 1
$$

其中$ W_2 $， 是卷积后 Feature Map 的宽度；$ W_1 $ 是卷积前图像的宽度；$ F $ 是 filter 的宽度；$ P $ 是 Zero Padding 数量，Zero Padding 是指在原始图像周围补几圈 $0$，如果 $P$ 的值是 $1$，那么就补 $1$ 圈 $0$；$S$ 是步幅；$ H_2 $ 卷积后 Feature Map 的高度；$ H_1 $ 是卷积前图像的宽度。

Q：CNN 权值共享问题

A：首先权值共享就是滤波器共享 ，滤波器的参数是固定的，即是用相同的滤波器去扫一遍图像，提取一次特征特征，得到feature map。在卷积网络中，学好了一个滤波器，就相当于掌握了一种特征，这个滤波器在图像中滑动，进行特征提取，然后所有进行这样操作的区域都会被采集到这种特征，就好比上面的水平线。
